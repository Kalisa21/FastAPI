{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install or upgrade datasets and fsspec\n",
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slypd6Fh3yaS",
        "outputId": "2a44d13e-9fa6-4ed4-858f-6d2bb2f6a565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"fancyzhx/amazon_polarity\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "dbcc52ebc5104626b4c66a85bc3db750",
            "b26d359fcee1489bb050aa901c6d77ae",
            "36bce5b1db61467a80d0cc491da2884d",
            "44e32eb449b84f53b710e3ecd2a40a75",
            "857ad0b71c4f4e6abeb72e0ef12a9ba0",
            "b57102ae73ed498faffbd57ba2345e57",
            "20532ce5a9814bc6bf9e7cbc3ac50e36",
            "bf727de57380466aaa8678fb45074615",
            "88ecbc3d81a848c0a200e4b0cac858de",
            "e027b62abce241c58b98b5248d97ea69",
            "04977709d85b424ca7e73293ef4a5d7f",
            "d880d459ec3f48f78b64c56c22fbf3be",
            "26049f3b9c4f4a46b87b458c81ca967d",
            "84d75da7a8c647c88a7644bc79c575a8",
            "1b566b5db33841a4b794a4017e361fb3",
            "a0d4db5ae4fd4927baa4ec1f6b6c3184",
            "eaa6a8e11fc1423ca34c9be4a3c48779",
            "41ddc368406d48afadf61faf0e91d289",
            "cf5c1d029d3b4727bb606fbc1409fda6",
            "28bdffa4cb804779965cc8723b97c34d",
            "63efc987375847b788fee06544b1bb1e",
            "d48850e359684f6881477963c57b2a8b"
          ]
        },
        "id": "WezAnBfI4HcM",
        "outputId": "f98962e1-6cab-43ee-89de-363a770454aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/6.81k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbcc52ebc5104626b4c66a85bc3db750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/260M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d880d459ec3f48f78b64c56c22fbf3be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApRx9tkM3SX0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's work with a smaller subset to prevent memory issues\n",
        "sample_size = 100000  # Adjust based on your system capabilities\n",
        "train_df = ds['train'].shuffle(seed=42).select(range(sample_size)).to_pandas()\n",
        "test_df = ds['test'].shuffle(seed=42).select(range(int(sample_size*0.2))).to_pandas()\n",
        "\n",
        "# Explore the data\n",
        "print(train_df.head())\n",
        "print(f\"\\nTrain shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
        "print(\"\\nLabel distribution in train set:\")\n",
        "print(train_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "JXxbCweY3njW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "Qkw11JYXDXqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# 1. Distribution of Review Length\n",
        "train_df['review_length'] = train_df['content'].apply(len)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(train_df['review_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Review Length (Characters)')\n",
        "plt.xlabel('Number of Characters')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(train_df['review_length'].mean(), color='r', linestyle='--', label=f'Mean: {train_df[\"review_length\"].mean():.0f} chars')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 2. Sentiment Class Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.countplot(x='label', data=train_df, palette='viridis')\n",
        "plt.title('Sentiment Class Distribution')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'])\n",
        "\n",
        "# Add percentages\n",
        "total = len(train_df)\n",
        "for p in ax.patches:\n",
        "    percentage = f'{100 * p.get_height()/total:.1f}%'\n",
        "    x = p.get_x() + p.get_width()/2\n",
        "    y = p.get_height() + 0.01*total\n",
        "    ax.annotate(percentage, (x, y), ha='center')\n",
        "plt.show()\n",
        "\n",
        "# 3. Top 20 Most Frequent Words\n",
        "def get_top_words(text_series, n=20):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    return word_counts.most_common(n)\n",
        "\n",
        "# Using processed text\n",
        "text_source = train_df['processed_text'] if 'processed_text' in train_df.columns else train_df['content']\n",
        "top_words = get_top_words(text_source)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "words, counts = zip(*top_words)\n",
        "sns.barplot(x=list(counts), y=list(words), palette='viridis')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Word')\n",
        "plt.show()\n",
        "\n",
        "# 4. Word Clouds by Sentiment\n",
        "def plot_wordcloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400,\n",
        "                         background_color='white',\n",
        "                         colormap='viridis').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title, size=15)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Positive reviews\n",
        "positive_text = ' '.join(train_df[train_df['label'] == 1]['content'])\n",
        "plot_wordcloud(positive_text, 'Most Common Words in Positive Reviews')\n",
        "\n",
        "# Negative reviews\n",
        "negative_text = ' '.join(train_df[train_df['label'] == 0]['content'])\n",
        "plot_wordcloud(negative_text, 'Most Common Words in Negative Reviews')"
      ],
      "metadata": {
        "id": "QWXa96T6CcHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "cnGAwsfG5n2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ObRIjOeF5vWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "7rEWEDsx57Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "NrJrUO6k6FJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    # Remove stopwords and lemmatize\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "BJpeDrqo6URN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "train_df['processed_text'] = train_df['content'].apply(preprocess_text)\n",
        "test_df['processed_text'] = test_df['content'].apply(preprocess_text)\n",
        "\n",
        "# Check processed text\n",
        "print(\"\\nSample processed text:\")\n",
        "print(train_df['processed_text'].iloc[0])"
      ],
      "metadata": {
        "id": "azlnbnsa6fnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization and Sequence Preparation"
      ],
      "metadata": {
        "id": "u1ICTF1y6x-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "max_words = 10000  # Vocabulary size\n",
        "max_len = 200      # Maximum sequence length\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_df['processed_text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = tokenizer.texts_to_sequences(train_df['processed_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['processed_text'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_df['label'].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "print(\"\\nSample sequence:\")\n",
        "print(X_train[0])\n",
        "print(f\"\\nSequence shapes - Train: {X_train.shape}, Test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "a2tp34jd61IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "x4zDWrwe7EJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "embedding_dim = 128\n",
        "lstm_units = 72\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim),  # Removed input_length\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "    Dropout(dropout_rate),\n",
        "    Bidirectional(LSTM(lstm_units)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Explicitly build the model with input shape\n",
        "model.build(input_shape=(None, max_len))  # This will properly initialize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "p2Bm-WZWlZvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced training parameters\n",
        "batch_size = 128  # Increased to speed up training\n",
        "epochs = 15  # Increased to allow more patience\n",
        "validation_split = 0.2\n",
        "\n",
        "# Enhanced early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # Increased patience\n",
        "    restore_best_weights=True,\n",
        "    min_delta=0.001  # Minimum improvement required\n",
        ")\n",
        "\n",
        "# Add learning rate reduction\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=8,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[early_stopping, reduce_lr],  # Added reduce_lr\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "h9RsAIyP6bca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision, Recall, and F1 Evaluation Cell\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score, precision_score, recall_score, classification_report\n",
        "import matplotlib.pyplot as plt # Ensure plt is imported for plotting\n",
        "import numpy as np # Ensure numpy is imported for array operations\n",
        "\n",
        "def evaluate_prf1(model, X_test, y_test):\n",
        "    # Generate predictions\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    ap = average_precision_score(y_test, y_pred_prob)  # Average precision\n",
        "\n",
        "    # Classification report with zero_division handled\n",
        "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'], zero_division=1)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Average Precision: {ap:.4f}\")\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR Curve (AP = {ap:.2f})')\n",
        "    plt.fill_between(recall_curve, precision_curve, alpha=0.2, color='blue')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # F1 Score Threshold Analysis\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "    f1_scores = [f1_score(y_test, (y_pred_prob > t).astype(int)) for t in thresholds]\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(thresholds, f1_scores, color='green', lw=2)\n",
        "    plt.axvline(x=0.5, color='red', linestyle='--', label='Default Threshold (0.5)')\n",
        "    plt.xlabel('Decision Threshold')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('F1 Score vs. Decision Threshold')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'average_precision': ap,\n",
        "        'classification_report': report,\n",
        "        'precision_curve': precision_curve,\n",
        "        'recall_curve': recall_curve,\n",
        "        'f1_thresholds': thresholds,\n",
        "        'f1_scores': f1_scores\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "prf1_results = evaluate_prf1(model, X_test, y_test)\n",
        "\n",
        "# Additional Interpretation\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"- Precision: Measures how many predicted positives are truly positive (Higher = fewer false positives)\")\n",
        "print(\"- Recall: Measures how many actual positives were caught (Higher = fewer false negatives)\")\n",
        "print(\"- F1: Harmonic mean of precision and recall (Best when both matter equally)\")\n",
        "print(\"- Average Precision: Summary of PR curve (Higher = better overall performance)\")\n",
        "print(\"- Optimal Threshold: Check where F1 peaks in right plot (may not be at 0.5)\")"
      ],
      "metadata": {
        "id": "8_88ZMOK562n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Cell with Probability Scores\n",
        "def predict_sentiment(model, tokenizer, reviews, max_len=200):\n",
        "    \"\"\"Predict sentiment for new reviews with probability scores\"\"\"\n",
        "\n",
        "    # Preprocess the new reviews\n",
        "    processed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "    # Convert text to sequences and pad them\n",
        "    sequences = tokenizer.texts_to_sequences(processed_reviews)\n",
        "    X_new = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    # Get predictions and probabilities\n",
        "    predictions = model.predict(X_new)\n",
        "    probs = np.concatenate([1 - predictions, predictions], axis=1)  # [prob_negative, prob_positive]\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"review\": review,\n",
        "            \"prediction\": \"POSITIVE\" if pred > 0.5 else \"NEGATIVE\",\n",
        "            \"confidence\": max(prob[1], 1 - prob[1]),  # Highest probability between the two classes\n",
        "            \"prob_negative\": float(prob[0]),  # Convert numpy float to Python float\n",
        "            \"prob_positive\": float(prob[1]),\n",
        "            \"interpretation\": \"Strong positive\" if prob[1] > 0.9 else\n",
        "                             \"Positive\" if prob[1] > 0.7 else\n",
        "                             \"Neutral\" if 0.4 <= prob[1] <= 0.6 else\n",
        "                             \"Negative\" if prob[1] > 0.3 else\n",
        "                             \"Strong negative\"\n",
        "        }\n",
        "        for review, pred, prob in zip(reviews, predictions, probs)\n",
        "    ]\n",
        "\n",
        "# Sample reviews to predict\n",
        "new_reviews = [\n",
        "    \"This product is amazing! Worth every penny.\",\n",
        "    \"Terrible quality, broke after 2 days.\",\n",
        "    \"It's okay but not what I expected.\",\n",
        "    \"Absolutely love it! Best purchase this year.\",\n",
        "    \"Waste of money. Doesn't work as advertised.\"\n",
        "]\n",
        "\n",
        "# Make predictions\n",
        "predictions = predict_sentiment(model, tokenizer, new_reviews)\n",
        "\n",
        "# Display results\n",
        "print(\"Sentiment Predictions with Confidence Scores:\")\n",
        "for i, pred in enumerate(predictions, 1):\n",
        "    print(f\"\\nReview {i}: {pred['review']}\")\n",
        "    print(f\"Prediction: {pred['prediction']} ({pred['interpretation']})\")\n",
        "    print(f\"Confidence: {pred['confidence']:.2%}\")\n",
        "    print(f\"P(Negative): {pred['prob_negative']:.4f}\")\n",
        "    print(f\"P(Positive): {pred['prob_positive']:.4f}\")"
      ],
      "metadata": {
        "id": "icsPtAuu7x4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Controversial Cases Evaluation Cell\n",
        "import pandas as pd\n",
        "\n",
        "def predict_with_confidence(model, tokenizer, reviews, max_len=200):\n",
        "    \"\"\"Predict sentiment with confidence scores for DataFrame output\"\"\"\n",
        "    processed_reviews = [preprocess_text(review) for review in reviews]\n",
        "    sequences = tokenizer.texts_to_sequences(processed_reviews)\n",
        "    X_new = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "    predictions = model.predict(X_new)\n",
        "    probs = np.concatenate([1 - predictions, predictions], axis=1)\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"review\": review,\n",
        "            \"prediction\": \"NEGATIVE\" if pred < 0.5 else \"POSITIVE\",\n",
        "            \"confidence\": max(prob[0], prob[1]),\n",
        "            \"prob_negative\": float(prob[0]),\n",
        "            \"prob_positive\": float(prob[1])\n",
        "        }\n",
        "        for review, pred, prob in zip(reviews, predictions, probs)\n",
        "    ]\n",
        "\n",
        "# Test controversial cases\n",
        "controversial = [\n",
        "    \"Not bad for the price\",\n",
        "    \"I don't hate it but would never recommend\",\n",
        "    \"Meh, it's fine\",\n",
        "    \"It works but I expected better\",\n",
        "    \"Good enough I guess\"\n",
        "]\n",
        "\n",
        "# Generate predictions and display as DataFrame\n",
        "controversial_predictions = predict_with_confidence(model, tokenizer, controversial)\n",
        "print(pd.DataFrame(controversial_predictions))\n",
        "\n",
        "# Add interpretation analysis\n",
        "print(\"\\nControversial Case Analysis:\")\n",
        "for idx, pred in enumerate(controversial_predictions, 1):\n",
        "    print(f\"\\nCase {idx}: '{pred['review']}'\")\n",
        "    print(f\"Model is {pred['confidence']:.1%} confident it's {pred['prediction']}\")\n",
        "    if 0.4 < pred['prob_positive'] < 0.6:\n",
        "        print(\"\u2192 This appears to be a truly ambiguous case\")\n",
        "    elif (pred['prediction'] == \"POSITIVE\" and pred['prob_negative'] > 0.3) or \\\n",
        "         (pred['prediction'] == \"NEGATIVE\" and pred['prob_positive'] > 0.3):\n",
        "        print(\"\u2192 Model shows uncertainty despite prediction\")"
      ],
      "metadata": {
        "id": "TDVNJw808Q_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Clear all output and problematic metadata\n",
        "display(Javascript('''\n",
        "    IPython.notebook.clear_all_output();\n",
        "    IPython.notebook.metadata.widgets = [];\n",
        "    IPython.notebook.save_notebook();\n",
        "    setTimeout(function() { alert(\"Notebook cleaned. Please save again.\"); }, 1000);\n",
        "'''))"
      ],
      "metadata": {
        "id": "93J1iTgZF7Ji"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}